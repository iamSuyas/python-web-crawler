import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
import threading
import logging
import re


class WebCrawler:
    def __init__(self,base_url):
        self.base_url = base_url.rstrip('/')
        self.visited_urls = set()
        self.urls_to_visit = [self.base_url]
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'
        }
        self.lock = threading.Lock()
        self.vulnerabilities = []
        
        
    def crawl(self):
        threads = []
        for _ in range(5):
            thread = threading.Thread(target=self.worker)
            thread.start()
            threads.append(thread)

        for thread in threads:
            thread.join()    
            
            
    def worker(self):
        while True:
            with self.lock:
                if not self.urls_to_visit:
                    return
                url = self.urls_to_visit.pop(0)

            if url not in self.visited_urls:
                self.visit_url(url)
                
                
    def visit_url(self, url):
        self.visited_urls.add(url)
        try:
            response = requests.get(url, headers=self.headers, timeout=10)
            print(response)
            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'html.parser')
                print(soup)
                self.check_security_headers(response, url)
                self.check_insecure_forms(soup, url)
                self.extract_links(soup, url)
        except requests.RequestException as e:
            logging.error(f"Failed to fetch {url}: {e}")


    def extract_links(self, soup, current_url):
        for tag in soup.find_all('a', href=True):
            href = urljoin(current_url, tag['href'])
            if urlparse(href).netloc == urlparse(self.base_url).netloc:
                with self.lock:
                    if href not in self.visited_urls and href not in self.urls_to_visit:
                        self.urls_to_visit.append(href)

    def check_security_headers(self, response, url):
        missing_headers = []
        required_headers = [
            'Strict-Transport-Security',
            'X-Content-Type-Options',
            'X-Frame-Options',
            'Content-Security-Policy'
        ]
        for header in required_headers:
            if header not in response.headers:
                missing_headers.append(header)

        if missing_headers:
            for h in missing_headers:
                msg = f"[{url}] Missing HTTP Security Header: {h}"
                self.vulnerabilities.append(msg)
                logging.info(msg)
                
                
                
    def check_insecure_forms(self, soup, url):
        forms = soup.find_all('form')
        for form in forms:
            method = form.attrs.get('method', 'get').lower()
            action = form.attrs.get('action', '').strip()

            if method == 'get' or not action:
                msg = f"[{url}] Insecure Form Detected (method={method}, action={'missing' if not action else action})"
                self.vulnerabilities.append(msg)
                logging.info(msg)
    


    
if __name__ == "__main__":
    base_url = input("Enter Website URL to scan: ").strip()
    if not base_url.startswith('http'):
        base_url = 'http://' + base_url
    crawler = WebCrawler(base_url)
    crawler.crawl()