import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
import threading
import logging
import re


class WebCrawler:
    def __init__(self,base_url):
        self.base_url = base_url.rstrip('/')
        self.visited_urls = set()
        self.urls_to_visit = [self.base_url]
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'
        }
        self.lock = threading.Lock()
        self.vulnerabilities = []
        
        
    def crawl(self):
        threads = []
        for _ in range(5):  # Adjust thread count for performance
            thread = threading.Thread(target=self.worker)
            thread.start()
            threads.append(thread)

        for thread in threads:
            thread.join()    
            
            
    def worker(self):
        while True:
            with self.lock:
                if not self.urls_to_visit:
                    return
                url = self.urls_to_visit.pop(0)

            if url not in self.visited_urls:
                self.visit_url(url)
                
                
    def visit_url(self, url):
        self.visited_urls.add(url)
        try:
            response = requests.get(url, headers=self.headers, timeout=10)
            print(response)
            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'html.parser')
                print(soup)
                # check for security haeders
                # check for insecure forms
                self.extract_links(soup, url)
        except requests.RequestException as e:
            logging.error(f"Failed to fetch {url}: {e}")


    def extract_links(self, soup, current_url):
        for tag in soup.find_all('a', href=True):
            href = urljoin(current_url, tag['href'])
            if urlparse(href).netloc == urlparse(self.base_url).netloc:
                with self.lock:
                    if href not in self.visited_urls and href not in self.urls_to_visit:
                        self.urls_to_visit.append(href)

if __name__ == "__main__":
    base_url = input("Enter Website URL to scan (e.g., https://example.com): ").strip()
    if not base_url.startswith('http'):
        base_url = 'http://' + base_url
    crawler = WebCrawler(base_url)
    crawler.crawl()